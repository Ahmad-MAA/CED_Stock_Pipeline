Go to Github web
create a new repo
go to vs code 
open a new terminal
run git init

create a .env file
create .gitignore
create REAMME.md file
Add the .env file in the gitignore file

run git config --global user.name "Ahmad-MAA"
run git config --global "musa.ahmad4abubakar@gmail.com"
run git status
git add .

 (use "git rm --cached <file>..." to unstage)
git status
git commit -m "first commit"

linking folder to repo
copy git remote add origin command e.g below
git remote add origin https://github.com/Ahmad-MAA/capital_edge_stock_data.git
git remote add origin https://github.com/Ahmad-MAA/capital_edge_stock_data.git
run copied 
run git push -u origin master

Create the coding environment  (.ipynb file)
import the libraries


run git status
run git add .
run git commit -m "Second  commit"
run git push -u origin master 

add the API in the .env
Create the EXtraction function 
Create the Transformation 
Create the Loading

then run the git command as above


Next is create a virtual environment 
open a new terminal 
run wsl
create a venv using 
python3 -m venv name_of_env
source ./name_of_env/bin/activate
pip install pandas
pip install azure_storage_blob
pip install python-dotenv
pip3 install apache-airflow

navigating to the virtual environment
run cd ~
run cd to envinronment location 
eg 
cd /c/Users/AHMAD/OneDrive/Documents/10alytics/main_capston


create the etl.py script
create the DAG script

place both scripts in the DAG folder of Airflow home
navigate to the Aiflow folder
Run the ls 
run sudo nano dag_script.py ////// copy the DAG script and Paste. 
do sampe for the etl.py


create a requirement.txt folder
run command pip install -r requirements.txt
install pacakeges individually

'''''''
sudo service mysql start
airflow db init

#create user account
# create a file and name it as below
create_user.txt
#type in the file (username admin is the name of the user it can change)# adminsef, esef
airflow users create --username adminsf --firstname ahmad --lastname musa --role Admin --email dad@gamil.com
#paste in the above in the terminal 
# save the password to remember 123
# 

run the create command and input password
#go to airflow directory from linux using 
cd /home/airflow
/c/Users/AHMAD/OneDrive/Documents/10alytics/main_capstone$
# run 
ls
# run / spind the server aiflow with 
airflow webserver --port 8080
8082
# run the airflow schedulere all on wsl from same directory cd/home/airflow
airflow scheduler
http://localhost:8080 on web pacakeges
log in on webpage

# open wsl >>> venv >> run
cd /home/ahmad/airflow

# make a new directory for the dag folder( mkdir zipco_food_dag)
mkdir zipco_food_dag

#run 
ls 
# run 
sudo nano airflow.cfg 
password 0852
chage dag file name to the new directory dag directory....(the last directory to the dag direcctory)
Ctrl + X, Y to save and enter to close

# change directory to the dag folder directory
# run
cd zipco_food_dag
open a new terminal  copy---  2
# opnen wsl console
copy file to airflow directory
cp zipco_transaction.csv /home/ahmad/airflow/zipco_food_dag
source new_env/bin/activate
cd /home/ahmad/airflow
cp .env /home/ahmad/airflow/zipco_food_dag
sudo nano .env ------- to check
copy all pipeline files
Ctrl C to end terminal

run / spind the server aiflow with webserver 8080
run the airflow schedulere all on wsl  virtual environment
end airflow pipeline

remove the file 
rm Extraction.py and other parts
# run 
sudo nano dag_script.py
copy from the .py file and paste the Ctrl + X Y enter(wsl venv aiflow dag folder)
and for other files
run ls
# do same for the etl script
~~~~ incase of failure 
email in dag script using sudo nano should be false 
airflow is reading the files from the wrong folder

#run 
find / -name file_name
# run 
locate file_name
if not work
#run 
locate sudo apt-get install mlocate
airflow list
source ./airflow_venv/bin/activate

