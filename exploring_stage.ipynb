{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary libraies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the Extaction of Data\n",
    "###### Looad the .env environment and \n",
    "###### Ensure all varibles are correct (API_KEY and SYMBOLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.environ.get(\"API_KEY\")\n",
    "SYMBOLS = ['AAPL', 'GOOG', 'MSFT', 'AMZN', 'IBM']\n",
    "\n",
    "def extract_data(symbols, api_key):\n",
    "   \n",
    "    all_rows = []\n",
    "\n",
    "    for symbol in symbols:\n",
    "        print(f\"Fetching data for {symbol}...\")\n",
    "        url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&apikey={api_key}'\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract time series data\n",
    "        try:\n",
    "            time_series = data['Time Series (Daily)']\n",
    "        except KeyError:\n",
    "            print(f\"No data found for {symbol}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Loop through the time series data\n",
    "        for date, values in time_series.items():\n",
    "            row = {\n",
    "                \"Date\": datetime.strptime(date, \"%Y-%m-%d\"),\n",
    "                \"Symbol\": symbol,\n",
    "                \"Open\": float(values[\"1. open\"]),\n",
    "                \"High\": float(values[\"2. high\"]),\n",
    "                \"Low\": float(values[\"3. low\"]),\n",
    "                \"Close\": float(values[\"4. close\"]),\n",
    "                \"Volume\": int(values[\"5. volume\"]),\n",
    "            }\n",
    "            all_rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for AAPL...\n",
      "Fetching data for GOOG...\n",
      "Fetching data for MSFT...\n",
      "Fetching data for AMZN...\n",
      "Fetching data for IBM...\n"
     ]
    }
   ],
   "source": [
    "df = extract_data(SYMBOLS, API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the Data\n",
    "######  Transform the extracted data by cleaning and deduplicating it.\n",
    "###### Removing a empty cell\n",
    "###### Dropping duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "    # Drop null values and duplicates\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df= df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>252.44</td>\n",
       "      <td>253.2800</td>\n",
       "      <td>249.4300</td>\n",
       "      <td>250.42</td>\n",
       "      <td>39480718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>252.23</td>\n",
       "      <td>253.5000</td>\n",
       "      <td>250.7500</td>\n",
       "      <td>252.20</td>\n",
       "      <td>35557542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-12-27</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>257.83</td>\n",
       "      <td>258.7000</td>\n",
       "      <td>253.0600</td>\n",
       "      <td>255.59</td>\n",
       "      <td>42355321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-12-26</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>258.19</td>\n",
       "      <td>260.1000</td>\n",
       "      <td>257.6300</td>\n",
       "      <td>259.02</td>\n",
       "      <td>27262983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-12-24</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>255.49</td>\n",
       "      <td>258.2100</td>\n",
       "      <td>255.2900</td>\n",
       "      <td>258.20</td>\n",
       "      <td>23234705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>2024-08-15</td>\n",
       "      <td>IBM</td>\n",
       "      <td>193.51</td>\n",
       "      <td>194.2500</td>\n",
       "      <td>193.2800</td>\n",
       "      <td>193.95</td>\n",
       "      <td>2471985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>2024-08-14</td>\n",
       "      <td>IBM</td>\n",
       "      <td>191.15</td>\n",
       "      <td>193.0900</td>\n",
       "      <td>190.7300</td>\n",
       "      <td>192.32</td>\n",
       "      <td>1895114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>2024-08-13</td>\n",
       "      <td>IBM</td>\n",
       "      <td>190.29</td>\n",
       "      <td>191.3100</td>\n",
       "      <td>189.2100</td>\n",
       "      <td>190.99</td>\n",
       "      <td>2178862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2024-08-12</td>\n",
       "      <td>IBM</td>\n",
       "      <td>191.25</td>\n",
       "      <td>191.5761</td>\n",
       "      <td>189.0001</td>\n",
       "      <td>189.48</td>\n",
       "      <td>2290421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2024-08-09</td>\n",
       "      <td>IBM</td>\n",
       "      <td>191.18</td>\n",
       "      <td>192.6300</td>\n",
       "      <td>189.0400</td>\n",
       "      <td>191.45</td>\n",
       "      <td>2773706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date Symbol    Open      High       Low   Close    Volume\n",
       "0   2024-12-31   AAPL  252.44  253.2800  249.4300  250.42  39480718\n",
       "1   2024-12-30   AAPL  252.23  253.5000  250.7500  252.20  35557542\n",
       "2   2024-12-27   AAPL  257.83  258.7000  253.0600  255.59  42355321\n",
       "3   2024-12-26   AAPL  258.19  260.1000  257.6300  259.02  27262983\n",
       "4   2024-12-24   AAPL  255.49  258.2100  255.2900  258.20  23234705\n",
       "..         ...    ...     ...       ...       ...     ...       ...\n",
       "495 2024-08-15    IBM  193.51  194.2500  193.2800  193.95   2471985\n",
       "496 2024-08-14    IBM  191.15  193.0900  190.7300  192.32   1895114\n",
       "497 2024-08-13    IBM  190.29  191.3100  189.2100  190.99   2178862\n",
       "498 2024-08-12    IBM  191.25  191.5761  189.0001  189.48   2290421\n",
       "499 2024-08-09    IBM  191.18  192.6300  189.0400  191.45   2773706\n",
       "\n",
       "[500 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of the File \n",
    "###### This session, the transformed data will be loaded to Azure blob storage in a CSV format. The loading is designed in such a way that the upload file will be read, a previous data will be identified, downloaded, temporarilly,  read, concacnated with the new data. A further cleaning will takeplace by  removing  duplicate, and dropping null values. This new data will now be renamed to the latest datetime stamp and  uploaded to the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### install the relevant libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "# Defining the storage parameters\n",
    "\n",
    "CONNECTION_STRING =os.environ.get(\"AZURE_CONNECTION_STRING\")\n",
    "CONTAINER_NAME = os.environ.get(\"AZURE_CONTAINER_NAME\")\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "#Loading the Data\n",
    "def load_data_to_azure(df, connection_string, container_name):\n",
    "    from azure.storage.blob import BlobServiceClient, BlobClient\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    import logging\n",
    "    import io\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    # Connect to Azure Blob Storage\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "    # Define the pattern for identifying previous files\n",
    "    file_prefix = \"stock_data_\"\n",
    "    file_extension = \".csv\"\n",
    "    previous_blob_name = None\n",
    "\n",
    "    # Fetch the latest blob with matching pattern\n",
    "    blobs = list(container_client.list_blobs())\n",
    "    matching_blobs = [blob.name for blob in blobs if blob.name.startswith(file_prefix) and blob.name.endswith(file_extension)]\n",
    "\n",
    "    if matching_blobs:\n",
    "        previous_blob_name = max(matching_blobs)  # Get the latest file (lexicographical order works with timestamps)\n",
    "\n",
    "    if previous_blob_name:\n",
    "        try:\n",
    "            logging.info(f\"Previous data file found: {previous_blob_name}\")\n",
    "            \n",
    "            # Download the existing blob content\n",
    "            previous_blob_client = container_client.get_blob_client(previous_blob_name)\n",
    "            blob_stream = io.BytesIO(previous_blob_client.download_blob().readall())\n",
    "            previous_data = pd.read_csv(blob_stream)\n",
    "\n",
    "            # Clean existing data\n",
    "            previous_data[\"Date\"] = pd.to_datetime(previous_data[\"Date\"])\n",
    "            previous_data.drop_duplicates(inplace=True)\n",
    "            previous_data.dropna(inplace=True)\n",
    "\n",
    "            # Concatenate with new data\n",
    "            combined_data = pd.concat([previous_data, df]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "            # Rename the existing blob\n",
    "            timestamp = datetime.now().strftime('%d%m%y_%H%M')\n",
    "            backup_blob_name = f\"{file_prefix}{timestamp}_backup{file_extension}\"\n",
    "            container_client.get_blob_client(previous_blob_name).start_copy_from_url(previous_blob_client.url)\n",
    "            previous_blob_client.delete_blob()\n",
    "            logging.info(f\"Previous blob renamed to: {backup_blob_name}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing previous blob: {e}\")\n",
    "            combined_data = df\n",
    "    else:\n",
    "        logging.info(\"No previous data file found. Creating a new one.\")\n",
    "        combined_data = df\n",
    "\n",
    "    # Upload the new file\n",
    "    current_timestamp = datetime.now().strftime('%d%m%y_%H%M')\n",
    "    new_blob_name = f\"{file_prefix}{current_timestamp}{file_extension}\"\n",
    "\n",
    "    # Save to a temporary buffer and upload\n",
    "    output_stream = io.BytesIO()\n",
    "    combined_data.to_csv(output_stream, index=False)\n",
    "    output_stream.seek(0)\n",
    "\n",
    "    try:\n",
    "        new_blob_client = container_client.get_blob_client(new_blob_name)\n",
    "        new_blob_client.upload_blob(output_stream, overwrite=True)\n",
    "        logging.info(f\"Data successfully uploaded to: {new_blob_name}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error uploading new blob: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://capitaledgestorage.blob.core.windows.net/real-data?restype=REDACTED&comp=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'Accept': 'application/xml'\n",
      "    'User-Agent': 'azsdk-python-storage-blob/12.23.1 Python/3.12.6 (Windows-10-10.0.19045-SP0)'\n",
      "    'x-ms-date': 'REDACTED'\n",
      "    'x-ms-client-request-id': 'e322c5a9-c877-11ef-ad60-448500d68147'\n",
      "    'Authorization': 'REDACTED'\n",
      "No body was attached to the request\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/xml'\n",
      "    'Server': 'Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0'\n",
      "    'x-ms-request-id': 'cb35d1b0-701e-0083-5d84-5c78c3000000'\n",
      "    'x-ms-client-request-id': 'e322c5a9-c877-11ef-ad60-448500d68147'\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'Date': 'Wed, 01 Jan 2025 19:37:58 GMT'\n",
      "INFO:root:No previous data file found. Creating a new one.\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Request URL: 'https://capitaledgestorage.blob.core.windows.net/real-data/stock_data_010125_1937.csv'\n",
      "Request method: 'PUT'\n",
      "Request headers:\n",
      "    'Content-Length': '26839'\n",
      "    'x-ms-blob-type': 'REDACTED'\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'Content-Type': 'application/octet-stream'\n",
      "    'Accept': 'application/xml'\n",
      "    'User-Agent': 'azsdk-python-storage-blob/12.23.1 Python/3.12.6 (Windows-10-10.0.19045-SP0)'\n",
      "    'x-ms-date': 'REDACTED'\n",
      "    'x-ms-client-request-id': 'e3a7a7e0-c877-11ef-a2f4-448500d68147'\n",
      "    'Authorization': 'REDACTED'\n",
      "A body is sent with the request\n",
      "INFO:azure.core.pipeline.policies.http_logging_policy:Response status: 201\n",
      "Response headers:\n",
      "    'Content-Length': '0'\n",
      "    'Content-MD5': 'REDACTED'\n",
      "    'Last-Modified': 'Wed, 01 Jan 2025 19:37:59 GMT'\n",
      "    'ETag': '\"0x8DD2A9BCC5AC4B4\"'\n",
      "    'Server': 'Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0'\n",
      "    'x-ms-request-id': 'cb35d23c-701e-0083-5f84-5c78c3000000'\n",
      "    'x-ms-client-request-id': 'e3a7a7e0-c877-11ef-a2f4-448500d68147'\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'x-ms-content-crc64': 'REDACTED'\n",
      "    'x-ms-request-server-encrypted': 'REDACTED'\n",
      "    'Date': 'Wed, 01 Jan 2025 19:37:59 GMT'\n",
      "INFO:root:Data successfully uploaded to: stock_data_010125_1937.csv\n"
     ]
    }
   ],
   "source": [
    "load_data_to_azure(transformed_df, CONNECTION_STRING, CONTAINER_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
